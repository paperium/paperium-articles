<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="/paperium-articles/assets/article_detail.css?ver=2"> 
    <link rel="stylesheet" href="/paperium-articles/assets/StyleSheet.css?ver=2" >

<title>When "Correct" Is Not Safe: Can We Trust Functionally Correc</title>

<meta name="keywords" content="Code agent security,  Functionally Correct yet Vulnerable (FCV) patches,  FCV-Attack,  LLM code generation vulnerabilities,  Autonomous bug fixing sec">

<meta name="description" content="Code agent security,  Functionally Correct yet Vulnerable (FCV) patches,  FCV-Attack,  LLM code generation vulnerabilities,  Autonomous bug fixing sec">

</head>
<body>
    <div class="container my-5">
        <div class="row">
    <div class="header-nav text-center">
        <a href="/paperium-articles/">Home</a> 
        <span class="separator">|</span> 
        <a href="#">Article</a> 
    </div>

    <div class="article-container">
        <div class="article-tags-row">
            <span class="article-category-badge ">Artificial Intelligence</span>
            <span class="article-category-badge">arXiv</span>
        </div>
        <article id="MainArticle" class="details-body customeArticle" itemscope itemtype="https://schema.org/Article">
            <h1 itemprop="headline" class="article-title">
                When "Correct" Is Not Safe: Can We Trust Functionally Correct Patches Generated
by Code Agents?
            </h1> 

            <div class="article-author-row">
                <span class="author-name">
                    <img src="/paperium-articles/assets/avatar.png" alt="Author" class="author-icon"> 
                    <span class="authorList">
                        Yibo Peng, James Song, Lei Li, Xinyu Yang, Mihai Christodorescu, Ravi Mangal, Corina Pasareanu, Haizhong Zheng, Beidi Chen
                    </span>
                </span>
            </div>
            <p class="publish-date">
                              23 Oct 2025&nbsp;&nbsp;&nbsp;&nbsp;  3 min read 
            </p>
                            <div class="article-image-wrapper">
                                <figure id="authorsImage"  >
                                    <img class="article-main-image" itemprop="image" src="https://paperium.net/Media/Articles/img/501_ceed1ab0-4866-4d6b-8ff4-18258aaab3d6.jpg" class="object-center object-cover" 
                                    alt="undefined" >
                                </figure>
                                 <p class="image-caption">AI-generated image, based on the article abstract</p>
                            </div>

            <div id="QuickInsightWapper" class="mb-4"  itemprop="abstract">
               <h2>
     
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Quick Insight
       </h2>
            <h3>When ‚ÄúCorrect‚Äù Code Hides a Secret Danger</h3>
<p>
Ever wondered if a bug‚Äëfree program could still be unsafe? <strong>Researchers have uncovered</strong> a sneaky problem: AI‚Äëdriven code assistants can produce patches that pass every test but secretly contain security holes. Imagine a locksmith who fixes a broken lock perfectly‚Äîyet leaves a hidden backdoor for thieves. That‚Äôs what the new ‚Äúfunctionally correct yet vulnerable‚Äù (FCV) patches do. <strong>These patches look flawless</strong> to the eyes of developers, but a single malicious query can turn them into a doorway for hackers. The study showed that popular AI models like ChatGPT and Claude, as well as tools such as SWE‚Äëagent and OpenHands, can be fooled with just one black‚Äëbox request, achieving a success rate of over 40‚ÄØ% on certain attacks. <strong>This discovery matters</strong> because millions of projects now rely on automated fixes from code agents, and a hidden flaw could expose sensitive data or cripple software. As we hand more coding tasks to AI, we must build security‚Äëaware safeguards‚Äîotherwise, a ‚Äúperfect‚Äù fix might be the most dangerous one of all. üåê
</p>
           </div> 
           <hr/>
  <div id="ShortReviewWapper" class="mb-4" itemprop="articleBody">
 <h2>
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Short Review
       </h2>
        <h2>Unveiling Functionally Correct yet Vulnerable Patches in Code Agents</h2>
<p>This insightful article addresses a critical, often overlooked security vulnerability in autonomous code agents. These agents are increasingly relied upon for bug fixing on platforms like GitHub. The core focus is on a novel threat termed <strong>Functionally Correct yet Vulnerable (FCV) patches</strong>, which deceptively pass all functional tests while secretly embedding exploitable code. The research introduces the <strong>FCV-Attack</strong>, a sophisticated black-box, single-query methodology. This attack demonstrates that leading Large Language Models (LLMs) and prominent agent scaffolds are universally susceptible. The study reveals significant FCV rates, with attacks propagating through internal model state contamination. This work fundamentally challenges existing security evaluation paradigms, urging a paradigm shift towards more comprehensive security assessments for AI-driven code generation and repair systems.</p>

<h2>Critical Evaluation of Code Agent Security</h2>

<h3>Strengths of the FCV Threat Analysis</h3>
<p>The article's primary strength lies in identifying and rigorously defining the novel concept of <strong>Functionally Correct yet Vulnerable (FCV) patches</strong>, addressing a significant blind spot in current code agent security evaluations. The robust <strong>FCV-Attack methodology</strong>, employing Common Weakness Enumeration (CWE)-based injections under a realistic black-box threat model, provides compelling evidence of widespread susceptibility across state-of-the-art LLMs and agent scaffolds. Clear metrics like FCV Rate and Attack Success Rate (ASR) quantify this critical security gap, revealing internal Key-Value (KV) cache contamination as a propagation mechanism.</p>

<h3>Weaknesses and Limitations</h3>
<p>While groundbreaking, the study primarily focuses on identifying and quantifying the FCV threat, with less emphasis on developing robust countermeasures. The finding that prompt-level defenses minimally reduce Attack Success Rate (ASR) suggests current mitigation strategies are insufficient. The observation that FCV rates inversely correlate with task complexity, being more pronounced in simpler bug fixes, warrants further investigation into its prevalence across varying task complexities. Deeper exploration into specific architectural vulnerabilities within LLMs facilitating internal model state contamination could also provide more targeted defense strategies.</p>

<h3>Implications for AI Security and Development</h3>
<p>The findings carry profound implications for autonomous code agent development. The revelation of <strong>FCV patches</strong> necessitates an urgent re-evaluation of security paradigms, moving beyond mere functional correctness. This research underscores the critical need for developing <strong>security-aware defenses</strong> to detect and prevent such stealthy vulnerabilities. It challenges the current trust placed in LLM-powered agents for critical tasks, urging the community to prioritize robust security measures and build more resilient, trustworthy AI systems in software engineering.</p>

<h2>Conclusion: A Call for Enhanced Code Agent Security</h2>
<p>This article makes a pivotal contribution to AI security by exposing a novel and significant threat: <strong>Functionally Correct yet Vulnerable (FCV) patches</strong>. Demonstrating widespread susceptibility of state-of-the-art LLMs and agent scaffolds, it highlights a critical oversight in current evaluation methodologies. The findings mandate developing advanced, <strong>security-aware defenses</strong> and a fundamental shift in assessing autonomous code generation systems' trustworthiness. This work is indispensable for AI development, software engineering, and cybersecurity professionals.</p>
  </div>
  <div id = "keywordsWapper" class="mb-4">
    <h3>Keywords</h3>
    <ul>
  <li>Code agent security</li><li> Functionally Correct yet Vulnerable (FCV) patches</li><li> FCV-Attack</li><li> LLM code generation vulnerabilities</li><li> Autonomous bug fixing security</li><li> AI agent security threats</li><li> SWE-Bench evaluation security</li><li> CWE-538 information exposure</li><li> Black-box AI attacks</li><li> Security-aware code agent development</li><li> Agent scaffold security</li><li> Vulnerable code patches</li><li> AI in software development security</li><li> GitHub code agent security</li><li> Overlooked AI security threats</li>
</ul>

  </div>
  <div id="linktoWebsiteWrapper">
   <p>
Read article comprehensive review in Paperium.net:
<a href="https://paperium.net/article/en/502/when-correct-is-not-safe-can-we-trust-functionally-correct-patches-generatedby-code-agents" target="_blank" title=" When "Correct" Is Not Safe: Can We Trust Functionally Correct Patches Generated
by Code Agents?">
    When "Correct" Is Not Safe: Can We Trust Functionally Correct Patches Generated
by Code Agents?
</a>
</p> 
 
</div>

<p class="mt-5">
    ü§ñ This analysis and review was primarily generated and structured by an AI . The content is provided for informational and quick-review purposes.
</p>
            <div class="my-5" id=similarWrapper>
       <h2 class="lead text-center mb-5" id="similarh2">Paperium AI Analysis & Review of Latest Scientific Research Articles </h2>
<h3 id="similarh3"> More Artificial Intelligence Article Reviews </h3>
       <div class="row row-cols-1 row-cols-md-2 row-cols-lg-3 g-4" id="articles-container" itemscope itemtype="https://schema.org/ItemList" >

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/279_9f507e2b-b502-4af3-bd6d-08ee2b4d45fd.jpg" class="card-img-top" alt="MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented
Generation Systems" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Jihao Zhao
          </div>
          <div class="article-meta-text">
            17 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/266-MoM-Mixtures-of-Scenario-Aware-Document-Memories-for-Retrieval-Augmented-Generation-Systems/index.html"  title="MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented
Generation Systems">
          <h3 class="card-title pb-2" itemprop="headline">MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented
Generation Systems</h3>
        </a>
        <a 
          href="/paperium-articles/articles/266-MoM-Mixtures-of-Scenario-Aware-Document-Memories-for-Retrieval-Augmented-Generation-Systems/index.html"
          title="MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented
Generation Systems"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/331_1ac57749-e7c7-4e57-a35d-7906ff6c436d.jpg" class="card-img-top" alt="GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for
Step-Level Reasoning" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Yao Zhang
          </div>
          <div class="article-meta-text">
            18 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/315-GroundedPRM-Tree-Guided-and-Fidelity-Aware-Process-Reward-Modeling-for-Step-Level-Reasoning/index.html"  title="GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for
Step-Level Reasoning">
          <h3 class="card-title pb-2" itemprop="headline">GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for
Step-Level Reasoning</h3>
        </a>
        <a 
          href="/paperium-articles/articles/315-GroundedPRM-Tree-Guided-and-Fidelity-Aware-Process-Reward-Modeling-for-Step-Level-Reasoning/index.html"
          title="GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for
Step-Level Reasoning"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/317_a8cc14c7-489c-49a8-94b5-46e5c85fca73.jpg" class="card-img-top" alt="SimKO: Simple Pass@K Policy Optimization" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Ruotian Peng
          </div>
          <div class="article-meta-text">
            18 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/301-SimKO-Simple-PassK-Policy-Optimization/index.html"  title="SimKO: Simple Pass@K Policy Optimization">
          <h3 class="card-title pb-2" itemprop="headline">SimKO: Simple Pass@K Policy Optimization</h3>
        </a>
        <a 
          href="/paperium-articles/articles/301-SimKO-Simple-PassK-Policy-Optimization/index.html"
          title="SimKO: Simple Pass@K Policy Optimization"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/523_f961027c-a9ad-4dc7-9e72-595554355e06.jpg" class="card-img-top" alt="Directional Reasoning Injection for Fine-Tuning MLLMs" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Chao Huang
          </div>
          <div class="article-meta-text">
            24 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/633-Directional-Reasoning-Injection-for-Fine-Tuning-MLLMs/index.html"  title="Directional Reasoning Injection for Fine-Tuning MLLMs">
          <h3 class="card-title pb-2" itemprop="headline">Directional Reasoning Injection for Fine-Tuning MLLMs</h3>
        </a>
        <a 
          href="/paperium-articles/articles/633-Directional-Reasoning-Injection-for-Fine-Tuning-MLLMs/index.html"
          title="Directional Reasoning Injection for Fine-Tuning MLLMs"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/316_d7b9eceb-58bc-4d12-93e4-b970a031a703.jpg" class="card-img-top" alt="VLA-0: Building State-of-the-Art VLAs with Zero Modification" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Ankit Goyal
          </div>
          <div class="article-meta-text">
            18 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/300-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification/index.html"  title="VLA-0: Building State-of-the-Art VLAs with Zero Modification">
          <h3 class="card-title pb-2" itemprop="headline">VLA-0: Building State-of-the-Art VLAs with Zero Modification</h3>
        </a>
        <a 
          href="/paperium-articles/articles/300-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification/index.html"
          title="VLA-0: Building State-of-the-Art VLAs with Zero Modification"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/323_1b10cbfc-f3a6-44b2-ab1e-b7fb517a68da.jpg" class="card-img-top" alt="The German Commons - 154 Billion Tokens of Openly Licensed Text for German
Language Models" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Lukas Gienapp
          </div>
          <div class="article-meta-text">
            18 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/307-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models/index.html"  title="The German Commons - 154 Billion Tokens of Openly Licensed Text for German
Language Models">
          <h3 class="card-title pb-2" itemprop="headline">The German Commons - 154 Billion Tokens of Openly Licensed Text for German
Language Models</h3>
        </a>
        <a 
          href="/paperium-articles/articles/307-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models/index.html"
          title="The German Commons - 154 Billion Tokens of Openly Licensed Text for German
Language Models"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

       </div>
   </div>
        </article>
        
 

     
    </div>
    </div>
    </div>
  <script src="/paperium-articles/assets/script1.js"></script>
 
</body>
</html>