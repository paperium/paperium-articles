<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="/paperium-articles/assets/article_detail.css?ver=2"> 
    <link rel="stylesheet" href="/paperium-articles/assets/StyleSheet.css?ver=2" >

<title>RL makes MLLMs see better than SFT</title>

<meta name="keywords" content="Multimodal Language Models (MLLMs),  vision encoder analysis,  MLLM training paradigms,  Reinforcement Learning for MLLMs,  Supervised Finetuning (SFT">

<meta name="description" content="Multimodal Language Models (MLLMs),  vision encoder analysis,  MLLM training paradigms,  Reinforcement Learning for MLLMs,  Supervised Finetuning (SFT">

</head>
<body>
    <div class="container my-5">
        <div class="row">
    <div class="header-nav text-center">
        <a href="/paperium-articles/">Home</a> 
        <span class="separator">|</span> 
        <a href="#">Article</a> 
    </div>

    <div class="article-container">
        <div class="article-tags-row">
            <span class="article-category-badge ">Artificial Intelligence</span>
            <span class="article-category-badge">arXiv</span>
        </div>
        <article id="MainArticle" class="details-body customeArticle" itemscope itemtype="https://schema.org/Article">
            <h1 itemprop="headline" class="article-title">
                RL makes MLLMs see better than SFT
            </h1> 

            <div class="article-author-row">
                <span class="author-name">
                    <img src="/paperium-articles/assets/avatar.png" alt="Author" class="author-icon"> 
                    <span class="authorList">
                        Junha Song, Sangdoo Yun, Dongyoon Han, Jaegul Choo, Byeongho Heo
                    </span>
                </span>
            </div>
            <p class="publish-date">
                              22 Oct 2025&nbsp;&nbsp;&nbsp;&nbsp;  3 min read 
            </p>
                            <div class="article-image-wrapper">
                                <figure id="authorsImage"  >
                                    <img class="article-main-image" itemprop="image" src="https://paperium.net/Media/Articles/img/418_c61bbf81-29b8-48cf-8e1f-fbce22a19c9f.jpg" class="object-center object-cover" 
                                    alt="undefined" >
                                </figure>
                                 <p class="image-caption">AI-generated image, based on the article abstract</p>
                            </div>

            <div id="QuickInsightWapper" class="mb-4"  itemprop="abstract">
               <h2>
     
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Quick Insight
       </h2>
            <h3>How Reinforcement Learning Helps AI See Better Than Traditional Training</h3>
<p>
Ever wondered why some AI can describe a photo with uncanny detail while others miss the obvious? <strong>Scientists discovered</strong> that a new training trick called reinforcement learning (RL) makes multimodal AI models ‚Äúsee‚Äù images far sharper than the older supervised finetuning (SFT) method. Think of it like teaching a child to recognize a dog by rewarding every correct guess, rather than just showing a textbook of dog pictures. This reward‚Äëbased learning sharpens the AI‚Äôs visual brain, letting it focus on the right parts of a picture‚Äîlike spotting a tiny bird on a distant branch. The result? AI that answers visual questions more accurately, even with far less training time. The researchers turned this insight into a simple recipe named PIVOT, which builds stronger ‚Äúeyes‚Äù for AI without the massive computing costs of traditional methods. <strong>Imagine</strong> your phone instantly understanding a scene with the precision of a seasoned photographer. <strong>This breakthrough</strong> shows that smarter training, not just bigger models, can bring us closer to truly perceptive machines. The future of AI vision just got a lot clearer. 
</p>
           </div> 
           <hr/>
  <div id="ShortReviewWapper" class="mb-4" itemprop="articleBody">
 <h2>
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Short Review
       </h2>
        <h2>Advancing Multimodal Language Models: A Deep Dive into Vision Encoder Optimization</h2>

<p>This insightful study addresses a critical gap in Multimodal Language Model (MLLM) research by investigating how post-training strategies fundamentally reshape the performance of their <strong>vision encoders</strong>. Challenging the assumption that MLLM capabilities primarily stem from the LLM backbone, the research meticulously compares Supervised Finetuning (SFT) with Reinforcement Learning (RL), specifically Direct Preference Optimization (DPO). Through diverse experiments, including Visual Question Answering (VQA) benchmarks, ImageNet classification, and gradient visualization, the authors demonstrate that RL-based training yields superior, more precisely localized visual representations. The work culminates in the introduction of <strong>PIVOT</strong> (Preference-Instructed Vision OpTimization), an efficient method that significantly enhances vision encoders, even outperforming larger, more computationally intensive models.</p>

<h2>Critical Evaluation</h2>

<h3>Strengths</h3>
<p>The study's primary strength lies in its novel focus on the often-overlooked <strong>vision encoder</strong> within MLLMs, providing much-needed empirical evidence for its critical role. The rigorous methodology, employing controlled comparisons between DPO and SFT across various tasks and scales, robustly demonstrates DPO's advantages in improving object localization and vision-language alignment. Furthermore, the introduction of the <strong>PIVOT method</strong> offers a practical, computationally efficient solution for building stronger vision backbones, representing a significant step forward for MLLM development.</p>

<h3>Weaknesses</h3>
<p>While the findings are compelling, the study could benefit from further exploration into the <strong>mechanistic understanding</strong> of how DPO precisely reshapes visual representations at a deeper architectural level. Additionally, while demonstrating clear advantages, the generalizability of PIVOT across an even broader spectrum of MLLM architectures and diverse real-world applications, beyond the evaluated benchmarks, warrants continued investigation to fully ascertain its universal applicability and <strong>long-term stability</strong>.</p>

<h3>Implications</h3>
<p>This research carries profound implications for the future of <strong>MLLM development</strong>, shifting the paradigm towards optimizing vision components rather than solely focusing on language models. By demonstrating the power of preference-based learning for vision encoders, it paves the way for more capable, efficient, and robust MLLMs, particularly in tasks requiring fine-grained visual understanding. The computational efficiency offered by PIVOT also suggests a more sustainable path for advancing these complex models, making high-performance MLLMs more accessible.</p>

<h2>Conclusion</h2>
<p>This article makes a substantial contribution to the field of multimodal AI, offering both a foundational understanding of how training strategies impact MLLM vision and a practical, innovative solution. By highlighting the critical role of the <strong>vision encoder</strong> and introducing PIVOT, the authors provide an effective and efficient recipe for building next-generation MLLMs. This work is poised to inspire further research into vision-centric optimization, ultimately leading to more powerful and resource-efficient AI systems.</p>
  </div>
  <div id = "keywordsWapper" class="mb-4">
    <h3>Keywords</h3>
    <ul>
  <li>Multimodal Language Models (MLLMs)</li><li> vision encoder analysis</li><li> MLLM training paradigms</li><li> Reinforcement Learning for MLLMs</li><li> Supervised Finetuning (SFT) in MLLMs</li><li> visual representations in MLLMs</li><li> Preference-Instructed Vision OpTimization (PIVOT)</li><li> advancing MLLM vision backbones</li><li> strong localized visual representations</li><li> MLLM downstream tasks performance</li><li> VQA benchmarks for MLLMs</li><li> efficient MLLM vision training</li><li> ImageNet classification with MLLMs</li><li> gradient visualization for MLLMs</li>
</ul>

  </div>
  <div id="linktoWebsiteWrapper">
   <p>
Read article comprehensive review in Paperium.net:
<a href="https://paperium.net/article/en/391/rl-makes-mllms-see-better-than-sft" target="_blank" title=" RL makes MLLMs see better than SFT">
    RL makes MLLMs see better than SFT
</a>
</p> 
 
</div>

<p class="mt-5">
    ü§ñ This analysis and review was primarily generated and structured by an AI . The content is provided for informational and quick-review purposes.
</p>
            <div class="my-5" id=similarWrapper>
       <h2 class="lead text-center mb-5" id="similarh2">Paperium AI Analysis & Review of Latest Scientific Research Articles </h2>
<h3 id="similarh3"> More Artificial Intelligence Article Reviews </h3>
       <div class="row row-cols-1 row-cols-md-2 row-cols-lg-3 g-4" id="articles-container" itemscope itemtype="https://schema.org/ItemList" >

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/766_4c3fa034-61a4-4e8a-9f0b-41c2f3ab73de.jpg" class="card-img-top" alt="Exploring Conditions for Diffusion models in Robotic Control" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Heeseong Shin
          </div>
          <div class="article-meta-text">
            31 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/863-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control/index.html"  title="Exploring Conditions for Diffusion models in Robotic Control">
          <h3 class="card-title pb-2" itemprop="headline">Exploring Conditions for Diffusion models in Robotic Control</h3>
        </a>
        <a 
          href="/paperium-articles/articles/863-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control/index.html"
          title="Exploring Conditions for Diffusion models in Robotic Control"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/268_e85c7e66-ad43-4afc-a4e3-a36d3380eb56.jpg" class="card-img-top" alt="VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Qunzhong Wang
          </div>
          <div class="article-meta-text">
            17 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/255-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning/index.html"  title="VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning">
          <h3 class="card-title pb-2" itemprop="headline">VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning</h3>
        </a>
        <a 
          href="/paperium-articles/articles/255-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning/index.html"
          title="VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/376_80df59e5-4d08-4d25-9cc2-1c9e70f24b74.jpg" class="card-img-top" alt="Build Your Personalized Research Group: A Multiagent Framework for Continual and
Interactive Science Automation" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Ed Li
          </div>
          <div class="article-meta-text">
            20 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/356-Build-Your-Personalized-Research-Group-A-Multiagent-Framework-for-Continual-and-Interactive-Scie/index.html"  title="Build Your Personalized Research Group: A Multiagent Framework for Continual and
Interactive Science Automation">
          <h3 class="card-title pb-2" itemprop="headline">Build Your Personalized Research Group: A Multiagent Framework for Continual and
Interactive Science Automation</h3>
        </a>
        <a 
          href="/paperium-articles/articles/356-Build-Your-Personalized-Research-Group-A-Multiagent-Framework-for-Continual-and-Interactive-Scie/index.html"
          title="Build Your Personalized Research Group: A Multiagent Framework for Continual and
Interactive Science Automation"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/444_9811feed-dfb7-48be-8597-5e3dba934ca4.jpg" class="card-img-top" alt="World-in-World: World Models in a Closed-Loop World" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Jiahan Zhang
          </div>
          <div class="article-meta-text">
            22 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/417-World-in-World-World-Models-in-a-Closed-Loop-World/index.html"  title="World-in-World: World Models in a Closed-Loop World">
          <h3 class="card-title pb-2" itemprop="headline">World-in-World: World Models in a Closed-Loop World</h3>
        </a>
        <a 
          href="/paperium-articles/articles/417-World-in-World-World-Models-in-a-Closed-Loop-World/index.html"
          title="World-in-World: World Models in a Closed-Loop World"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/518_fc1b7ddb-868e-4bde-9dda-169930a57348.jpg" class="card-img-top" alt="Attention Sinks in Diffusion Language Models" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Maximo Eduardo Rulli
          </div>
          <div class="article-meta-text">
            24 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/628-Attention-Sinks-in-Diffusion-Language-Models/index.html"  title="Attention Sinks in Diffusion Language Models">
          <h3 class="card-title pb-2" itemprop="headline">Attention Sinks in Diffusion Language Models</h3>
        </a>
        <a 
          href="/paperium-articles/articles/628-Attention-Sinks-in-Diffusion-Language-Models/index.html"
          title="Attention Sinks in Diffusion Language Models"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/435_2c073d04-4942-42df-9c86-c8e28b48ed1e.jpg" class="card-img-top" alt="Balanced Multi-Task Attention for Satellite Image Classification: A Systematic
Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Aditya Vir
          </div>
          <div class="article-meta-text">
            22 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/408-Balanced-Multi-Task-Attention-for-Satellite-Image-Classification-A-Systematic-Approach-to-Achiev/index.html"  title="Balanced Multi-Task Attention for Satellite Image Classification: A Systematic
Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training">
          <h3 class="card-title pb-2" itemprop="headline">Balanced Multi-Task Attention for Satellite Image Classification: A Systematic
Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training</h3>
        </a>
        <a 
          href="/paperium-articles/articles/408-Balanced-Multi-Task-Attention-for-Satellite-Image-Classification-A-Systematic-Approach-to-Achiev/index.html"
          title="Balanced Multi-Task Attention for Satellite Image Classification: A Systematic
Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

       </div>
   </div>
        </article>
        
 

     
    </div>
    </div>
    </div>
  <script src="/paperium-articles/assets/script1.js"></script>
 
</body>
</html>