<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="/paperium-articles/assets/article_detail.css"> 

<title>D2E: Scaling Vision-Action Pretraining on Desktop Data for T</title>

<meta name="keywords" content="large language models,  embodied AI,  desktop interactions,  sensorimotor learning,  D2E framework,  OWA Toolkit,  Generalist-IDM,  zero-shot generali">

<meta name="description" content="large language models,  embodied AI,  desktop interactions,  sensorimotor learning,  D2E framework,  OWA Toolkit,  Generalist-IDM,  zero-shot generali">

</head>
<body>
    <div class="container my-5">
        <div class="row">
    <div class="header-nav text-center">
        <a href="/">Home</a> 
        <span class="separator">|</span> 
        <a href="#">Article</a> 
    </div>

    <div class="article-container">
        <div class="article-tags-row">
            <span class="article-category-badge ">Artificial Intelligence</span>
            <span class="article-category-badge">arXiv</span>
        </div>
        <article id="MainArticle" class="details-body customeArticle" itemscope itemtype="https://schema.org/Article">
            <h1 itemprop="headline" class="article-title">
                D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied
AI
            </h1> 

            <div class="article-author-row">
                <span class="author-name">
                    <img src="/assets/avatar.png" alt="Author" class="author-icon"> 
                    <span class="authorList">
                        Suwhan Choi, Jaeyoon Jung, Haebin Seong, Minchan Kim, Minyeong Kim, Yongjun Cho, Yoonshik Kim, Yubeen Park, Youngjae Yu, Yunsung Lee
                    </span>
                </span>
            </div>
            <p class="publish-date">
                              13 Oct 2025&nbsp;&nbsp;&nbsp;&nbsp;  3 min read 
            </p>
                            <div class="article-image-wrapper">
                                <figure id="authorsImage"  >
                                    <img class="article-main-image" itemprop="image" src="https://paperium.net/Media/Articles/img/79_1de8f843-9fc1-4119-9ffc-d19feeecb1f2.jpg" class="object-center object-cover" 
                                    alt="undefined" >
                                </figure>
                                 <p class="image-caption">AI-generated image, based on the article abstract</p>
                            </div>

            <div id="QuickInsightWapper" class="mb-4"  itemprop="abstract">
               <h2>
     
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Quick Insight
       </h2>
            <h3>From Video Games to Real‚ÄëWorld Robots: The Surprising Shortcut</h3>
<p>
Ever wondered how a robot could learn to pick up a cup without ever touching one? <strong>Scientists discovered</strong> that the key lies in the countless hours we spend playing games on our computers. By watching a virtual hand move in a game, a robot can pick up the same skill in the real world. Think of it like a child learning to ride a bike by watching a cartoon‚Äîonce they see the motions, they can try it themselves. Our team built a system called D2E that gathers millions of gameplay moments, compresses them, and teaches robots the basics of moving and grabbing. The result? Robots that succeed over 96% of the time in a lab‚Äëtest of object manipulation and 83% in navigation challenges, all thanks to ‚Äúdesktop training.‚Äù <strong>This breakthrough</strong> shows that the digital playground is a cheap, endless classroom for future machines. <strong>Imagine</strong> a world where every new video game instantly makes robots smarter, bringing us closer to helpful assistants at home and work. The adventure has just begun.
</p>
           </div> 
           <hr/>
  <div id="ShortReviewWapper" class="mb-4" itemprop="articleBody">
 <h2>
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Short Review
       </h2>
        <h2>Overview</h2>
<p>The article introduces the D2E (Desktop to Embodied AI) framework, which innovatively utilizes desktop data for pretraining <strong>embodied AI</strong>. This approach addresses the challenges associated with the high costs of physical trajectory collection by leveraging rich sensorimotor interactions found in desktop environments, particularly gaming. The framework comprises three main components: the OWA Toolkit for standardized data capture, the Generalist-IDM for effective event prediction and pseudo-labeling, and VAPT for transferring learned representations to robotics. The findings demonstrate impressive success rates of 96.6% in LIBERO manipulation and 83.3% in CANVAS navigation tasks, validating the potential of desktop pretraining for robotics applications.</p>

<h3>Critical Evaluation</h3>
<h3>Strengths</h3>
<p>The D2E framework presents several notable strengths, particularly its comprehensive approach to data collection and processing. The OWA Toolkit significantly enhances data storage efficiency, achieving a 152x compression rate, which is crucial for managing large datasets. Additionally, the Generalist-IDM's ability to perform <strong>zero-shot generalization</strong> across diverse gaming environments showcases its robustness and adaptability. The high success rates in manipulation and navigation tasks further underscore the framework's effectiveness in transferring knowledge from desktop interactions to real-world robotics.</p>

<h3>Weaknesses</h3>
<p>Despite its strengths, the article does have some limitations. The reliance on desktop environments may not fully capture the complexities of real-world interactions, potentially limiting the generalizability of the findings. Furthermore, while the framework demonstrates high performance, the implications of human supervision in data collection raise questions about scalability and the potential biases introduced by human input. Addressing these concerns will be essential for broader applicability in diverse robotic tasks.</p>

<h3>Implications</h3>
<p>The implications of this research are significant for the field of <strong>embodied AI</strong>. By establishing a practical paradigm for pretraining using desktop data, the D2E framework opens new avenues for scalable AI development. The availability of comprehensive resources, including the OWA Toolkit and datasets, promotes reproducibility and encourages further exploration in this area. This work could inspire future research to refine and expand upon the methodologies presented, ultimately enhancing the capabilities of robotics in various applications.</p>

<h2>Conclusion</h2>
<p>In summary, the D2E framework represents a substantial advancement in the field of embodied AI, effectively utilizing desktop interactions for pretraining. The article's findings highlight the potential of this approach to improve performance in robotics tasks while providing valuable resources for the research community. As the field continues to evolve, the insights gained from this study will likely influence future developments in <strong>robotics</strong> and AI, paving the way for more sophisticated and capable systems.</p>

<h2>Readability</h2>
<p>The article is well-structured and presents complex ideas in a clear and accessible manner. The use of concise paragraphs and straightforward language enhances readability, making it easier for a professional audience to engage with the content. By focusing on key findings and implications, the article effectively communicates the significance of the D2E framework in advancing embodied AI research.</p>
  </div>
  <div id = "keywordsWapper" class="mb-4">
    <h3>Keywords</h3>
    <ul>
  <li>large language models</li><li> embodied AI</li><li> desktop interactions</li><li> sensorimotor learning</li><li> D2E framework</li><li> OWA Toolkit</li><li> Generalist-IDM</li><li> zero-shot generalization</li><li> pseudo-labeling</li><li> robotics pretraining</li><li> physical manipulation</li><li> navigation benchmarks</li><li> data collection pipeline</li><li> human demonstrations</li><li> scalable desktop data</li>
</ul>

  </div>
  <div id="linktoWebsiteWrapper">
   <p>
Read article comprehensive review in Paperium.net:
<a href="https://paperium.net/article/en/75/d2e-scaling-vision-action-pretraining-on-desktop-data-for-transfer-to-embodiedai" target="_blank" title=" D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied
AI">
    D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied
AI
</a>
</p> 
 
</div>
<p class="mt-5">
    ü§ñ This analysis and review was primarily generated and structured by an AI . The content is provided for informational and quick-review purposes.
</p>
        </article>
        
 

     
    </div>
    </div>
    </div>
  <link rel="stylesheet" href="/paperium-articles/assets/script1.js"> 
 
</body>
</html>