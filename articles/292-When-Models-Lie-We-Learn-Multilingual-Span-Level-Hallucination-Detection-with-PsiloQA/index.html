<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="/paperium-articles/assets/article_detail.css"> 
    <link rel="stylesheet" href="/paperium-articles/assets/StyleSheet.css"  />

<title>When Models Lie, We Learn: Multilingual Span-Level Hallucina</title>

<meta name="keywords" content="LLM hallucination detection,  Multilingual hallucination detection,  Span-level hallucination annotation,  PsiloQA dataset,  Large language model reli">

<meta name="description" content="LLM hallucination detection,  Multilingual hallucination detection,  Span-level hallucination annotation,  PsiloQA dataset,  Large language model reli">

</head>
<body>
    <div class="container my-5">
        <div class="row">
    <div class="header-nav text-center">
        <a href="/paperium-articles/">Home</a> 
        <span class="separator">|</span> 
        <a href="#">Article</a> 
    </div>

    <div class="article-container">
        <div class="article-tags-row">
            <span class="article-category-badge ">Artificial Intelligence</span>
            <span class="article-category-badge">arXiv</span>
        </div>
        <article id="MainArticle" class="details-body customeArticle" itemscope itemtype="https://schema.org/Article">
            <h1 itemprop="headline" class="article-title">
                When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with
PsiloQA
            </h1> 

            <div class="article-author-row">
                <span class="author-name">
                    <img src="/paperium-articles/assets/avatar.png" alt="Author" class="author-icon"> 
                    <span class="authorList">
                        Elisei Rykov, Kseniia Petrushina, Maksim Savkin, Valerii Olisov, Artem Vazhentsev, Kseniia Titova, Alexander Panchenko, Vasily Konovalov, Julia Belikova
                    </span>
                </span>
            </div>
            <p class="publish-date">
                              18 Oct 2025&nbsp;&nbsp;&nbsp;&nbsp;  3 min read 
            </p>
                            <div class="article-image-wrapper">
                                <figure id="authorsImage"  >
                                    <img class="article-main-image" itemprop="image" src="https://paperium.net/Media/Articles/img/308_a1cd3bc7-777e-4d72-9505-6d31ff36e667.jpg" class="object-center object-cover" 
                                    alt="undefined" >
                                </figure>
                                 <p class="image-caption">AI-generated image, based on the article abstract</p>
                            </div>

            <div id="QuickInsightWapper" class="mb-4"  itemprop="abstract">
               <h2>
     
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Quick Insight
       </h2>
            <h3>How a New Multilingual Test Is Teaching AI to Stop Making Up Facts</h3>
<p>
Ever wondered why AI sometimes makes up facts? A new breakthrough called <strong>PsiloQA</strong> is changing that. Researchers have built a massive multilingual test that spots those madeâ€‘up bits right down to the exact words, and it works in 14 languages. Think of it like a spellâ€‘checker for truth, catching errors the moment they appear, no matter if the AI is answering in English, Spanish or Swahili.<br><br>
The team used clever automation: first they let a smart model write questionâ€‘answer pairs from Wikipedia, then they asked other AIs to answer without any hints, and finally a powerful system compared the replies to the real facts, marking the false fragments. Whatâ€™s exciting is that simple encoder models trained on this data became the best detectives for <strong>hallucination detection</strong>, even helping other tests become more reliable, all while costing far less than hiring humans.<br><br>
This means future chatbots and search tools will be less likely to lead us astray, making everyday information safer and more trustworthy. Imagine asking your phone for medical advice in Hindi and getting a reliable answerâ€”thanks to this work, that future feels closer. As AI spreads across the globe, tools like <strong>multilingual hallucination detection</strong> keep the promise of technology honest, reminding us that progress is only as good as its truth. Stay curious, and watch the AI world get smarter, not sillier.
</p>
           </div> 
           <hr/>
  <div id="ShortReviewWapper" class="mb-4" itemprop="articleBody">
 <h2>
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Short Review
       </h2>
        <h2>Overview</h2>
<p>This article introduces PsiloQA, a novel, large-scale <strong>multilingual dataset</strong> for fine-grained, span-level <strong>hallucination detection</strong> in Large Language Models (LLMs). It addresses limitations of existing sequence-level, English-only benchmarks by providing annotations across 14 languages. The dataset is constructed via an automated three-stage GPT-4o pipeline, generating Q&A pairs, eliciting hallucinations, and precisely annotating spans.</p>
<p>Research shows <strong>encoder-based models</strong>, particularly fine-tuned mmBERT, achieve superior cross-lingual performance. PsiloQA also demonstrates effective <strong>cross-lingual generalization</strong> and knowledge transfer, proving a cost-efficient alternative to human-annotated datasets. This work significantly advances scalable and precise hallucination detection in multilingual settings.</p>

<h2>Critical Evaluation</h2>
<h3>Strengths</h3>
<p>The introduction of PsiloQA is a major strength, offering a <strong>large-scale, multilingual, span-level dataset</strong> that fills critical gaps in LLM evaluation. Its automated GPT-4o pipeline ensures scalability and cost-effectiveness. Manual validation confirms GPT-4o's reliability for span-level annotation, bolstering dataset quality.</p>
<p>Empirical findings highlight the superior performance of <strong>fine-tuned encoder models</strong> across 14 languages, guiding future detection methodologies. PsiloQA's demonstrated cross-lingual generalization and knowledge transfer further enhance its value.</p>

<h3>Weaknesses</h3>
<p>A key limitation is the potential for <strong>GPT-4o bias</strong> in dataset generation and annotation, which could influence the types of hallucinations captured. This inherent bias might affect the dataset's representativeness of real-world LLM errors. Future work should explore methods to mitigate this bias.</p>
<p>Another challenge lies in achieving precise <strong>span-level boundary detection</strong>, indicated by relatively lower IoU scores. While the dataset offers fine-grained annotations, models still struggle with exact localization, pointing to an area for methodological improvement.</p>

<h3>Implications</h3>
<p>This research holds significant implications for the <strong>safe and reliable deployment</strong> of LLMs, especially in applications requiring factual accuracy. By enabling precise, multilingual hallucination detection, it directly contributes to building more trustworthy AI systems. PsiloQA offers a crucial tool for benchmarking and improving LLM robustness across diverse linguistic contexts.</p>
<p>The dataset's strong <strong>cross-lingual generalization</strong> is vital for advancing global AI accessibility. It provides a foundation for developing LLMs that perform reliably across languages, accelerating research into more robust detection strategies.</p>

<h2>Conclusion</h2>
<p>This article presents a significant contribution by introducing PsiloQA, a groundbreaking <strong>multilingual, span-level hallucination dataset</strong>. Its automated construction and comprehensive evaluation offer a scalable, cost-effective solution to a critical problem. The findings underscore the efficacy of encoder-based models and the dataset's strong cross-lingual capabilities, marking a substantial step forward in ensuring LLM factual integrity and reliability in diverse linguistic environments. This research is essential for anyone focused on enhancing the safety and performance of advanced AI through robust <strong>hallucination detection systems</strong>.</p>
  </div>
  <div id = "keywordsWapper" class="mb-4">
    <h3>Keywords</h3>
    <ul>
  <li>LLM hallucination detection</li><li> Multilingual hallucination detection</li><li> Span-level hallucination annotation</li><li> PsiloQA dataset</li><li> Large language model reliability</li><li> Automated dataset generation</li><li> GPT-4o for annotation</li><li> Encoder-based detection models</li><li> Cross-lingual generalization LLMs</li><li> Factual accuracy in LLMs</li><li> Uncertainty quantification LLMs</li><li> Safe LLM deployment</li><li> Fine-grained hallucination detection</li><li> Knowledge transfer benchmarks</li>
</ul>

  </div>
  <div id="linktoWebsiteWrapper">
   <p>
Read article comprehensive review in Paperium.net:
<a href="https://paperium.net/article/en/292/when-models-lie-we-learn-multilingual-span-level-hallucination-detection-withpsiloqa" target="_blank" title=" When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with
PsiloQA">
    When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with
PsiloQA
</a>
</p> 
 
</div>

<p class="mt-5">
    ðŸ¤– This analysis and review was primarily generated and structured by an AI . The content is provided for informational and quick-review purposes.
</p>
            <div class="my-5">
       <h2 class="lead text-center mb-5">Paperium AI Analysis & Review of Latest Scientific Research Articles </h2>
<h3>More Artificial Intelligence Article Reviews </h3>
       <div class="row row-cols-1 row-cols-md-2 row-cols-lg-3 g-4" id="articles-container" itemscope itemtype="https://schema.org/ItemList" >

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/68_c4b33efc-ff72-44df-9e82-546e8ae8da2e.jpg" class="card-img-top" alt="Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Yuntao Gui
          </div>
          <div class="article-meta-text">
            13 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/55-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models/index.html"  title="Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models">
          <h3 class="card-title pb-2" itemprop="headline">Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models</h3>
        </a>
        <a 
          href="/paperium-articles/articles/55-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models/index.html"
          title="Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/46_d787a0ff-de6b-4f1f-8aa0-885100ebfeb1.jpg" class="card-img-top" alt="NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models
under Data Constraints" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Changyao Tian
          </div>
          <div class="article-meta-text">
            13 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/37-NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constra/index.html"  title="NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models
under Data Constraints">
          <h3 class="card-title pb-2" itemprop="headline">NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models
under Data Constraints</h3>
        </a>
        <a 
          href="/paperium-articles/articles/37-NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constra/index.html"
          title="NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models
under Data Constraints"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/28_537212e2-9fa3-45e1-a3a4-3c2c497d2d67.jpg" class="card-img-top" alt="MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic
Platform and Adaptive Hybrid Policy Optimization" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Xiangyu Zhao
          </div>
          <div class="article-meta-text">
            13 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/19-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive/index.html"  title="MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic
Platform and Adaptive Hybrid Policy Optimization">
          <h3 class="card-title pb-2" itemprop="headline">MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic
Platform and Adaptive Hybrid Policy Optimization</h3>
        </a>
        <a 
          href="/paperium-articles/articles/19-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive/index.html"
          title="MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic
Platform and Adaptive Hybrid Policy Optimization"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/80_cfc7cfed-d2b0-46f1-b873-4fbf359f34a8.jpg" class="card-img-top" alt="TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion
Sampling" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Hyunmin Cho
          </div>
          <div class="article-meta-text">
            13 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/76-TAGTangential-Amplifying-Guidance-for-Hallucination-Resistant-Diffusion-Sampling/index.html"  title="TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion
Sampling">
          <h3 class="card-title pb-2" itemprop="headline">TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion
Sampling</h3>
        </a>
        <a 
          href="/paperium-articles/articles/76-TAGTangential-Amplifying-Guidance-for-Hallucination-Resistant-Diffusion-Sampling/index.html"
          title="TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion
Sampling"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/35_a3272375-53f4-457a-b0ea-940e2e3f582c.jpg" class="card-img-top" alt="When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Soyeong Jeong
          </div>
          <div class="article-meta-text">
            13 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/26-When-Thoughts-Meet-Facts-Reusable-Reasoning-for-Long-Context-LMs/index.html"  title="When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs">
          <h3 class="card-title pb-2" itemprop="headline">When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs</h3>
        </a>
        <a 
          href="/paperium-articles/articles/26-When-Thoughts-Meet-Facts-Reusable-Reasoning-for-Long-Context-LMs/index.html"
          title="When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/187_855bdde7-49d3-4ebd-ad7e-c4971ce78eeb.jpg" class="card-img-top" alt="World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World
Knowledge" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Moo Hyun Son
          </div>
          <div class="article-meta-text">
            14 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/176-World-To-Image-Grounding-Text-to-Image-Generation-with-Agent-Driven-World-Knowledge/index.html"  title="World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World
Knowledge">
          <h3 class="card-title pb-2" itemprop="headline">World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World
Knowledge</h3>
        </a>
        <a 
          href="/paperium-articles/articles/176-World-To-Image-Grounding-Text-to-Image-Generation-with-Agent-Driven-World-Knowledge/index.html"
          title="World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World
Knowledge"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

       </div>
   </div>
        </article>
        
 

     
    </div>
    </div>
    </div>
  <script src="/paperium-articles/assets/script1.js"></script>
 
</body>
</html>