<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="/paperium-articles/assets/article_detail.css?ver=2"> 
    <link rel="stylesheet" href="/paperium-articles/assets/StyleSheet.css?ver=2" >

<title>Tongyi DeepResearch Technical Report</title>

<meta name="keywords" content="agentic large language model for deep research,  long‚Äëhorizon information‚Äëseeking tasks,  agentic mid‚Äëtraining fine‚Äëtuning,  agentic post‚Äëtraining aut">

<meta name="description" content="agentic large language model for deep research,  long‚Äëhorizon information‚Äëseeking tasks,  agentic mid‚Äëtraining fine‚Äëtuning,  agentic post‚Äëtraining aut">

</head>
<body>
    <div class="container my-5">
        <div class="row">
    <div class="header-nav text-center">
        <a href="/paperium-articles/">Home</a> 
        <span class="separator">|</span> 
        <a href="#">Article</a> 
    </div>

    <div class="article-container">
        <div class="article-tags-row">
            <span class="article-category-badge ">Artificial Intelligence</span>
            <span class="article-category-badge">arXiv</span>
        </div>
        <article id="MainArticle" class="details-body customeArticle" itemscope itemtype="https://schema.org/Article">
            <h1 itemprop="headline" class="article-title">
                Tongyi DeepResearch Technical Report
            </h1> 

            <div class="article-author-row">
                <span class="author-name">
                    <img src="/paperium-articles/assets/avatar.png" alt="Author" class="author-icon"> 
                    <span class="authorList">
                        Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang
                    </span>
                </span>
            </div>
            <p class="publish-date">
                              29 Oct 2025&nbsp;&nbsp;&nbsp;&nbsp;  3 min read 
            </p>
                            <div class="article-image-wrapper">
                                <figure id="authorsImage"  >
                                    <img class="article-main-image" itemprop="image" src="https://paperium.net/Media/Articles/img/693_51af6959-0309-4513-a33a-37380bf7265d.jpg" class="object-center object-cover" 
                                    alt="undefined" >
                                </figure>
                                 <p class="image-caption">AI-generated image, based on the article abstract</p>
                            </div>

            <div id="QuickInsightWapper" class="mb-4"  itemprop="abstract">
               <h2>
     
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Quick Insight
       </h2>
            <h3>Meet Tongyi DeepResearch: The AI That Can Do Your Homework for You</h3>
<p>
Ever wished a computer could dive into the internet and bring back exactly the answers you need? <strong>Scientists have built</strong> a new AI called <strong>Tongyi DeepResearch</strong> that does just that. Imagine a super‚Äëcurious librarian who never sleeps, skimming millions of pages in seconds to find the perfect piece of information‚Äîthis model works like that, but inside your phone or laptop. It was trained using a clever ‚Äúself‚Äëteaching‚Äù method, so it learns to ask the right questions and chase down clues without humans writing every rule. The result? A powerful <strong>large language model</strong> that can handle long‚Äëterm research projects, from solving tricky exams to exploring complex web topics, all while using only a fraction of its brainpower for each step.<br><br>
What does this mean for everyday life? Faster, more reliable answers for students, journalists, and anyone curious about the world. As this technology opens up, we‚Äôre stepping into a future where deep knowledge is just a click away. üåü
</p>
           </div> 
           <hr/>
  <div id="ShortReviewWapper" class="mb-4" itemprop="articleBody">
 <h2>
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Short Review
       </h2>
        <h2>Advancing Agentic LLMs for Deep Information-Seeking Research</h2>

<p>This article introduces Tongyi DeepResearch, an innovative <strong>agentic Large Language Model</strong> (LLM) specifically engineered for complex, long-horizon information-seeking research tasks. The authors detail an advanced end-to-end training framework that integrates agentic mid-training and post-training, enabling scalable reasoning and information retrieval. A key methodological innovation is the fully automatic, highly scalable data synthesis pipeline, which generates high-quality synthetic data without relying on costly human annotation. This system leverages customized environments to ensure stable and consistent interactions throughout its training stages. Tongyi DeepResearch, featuring 30.5 billion total parameters, achieves <strong>state-of-the-art performance</strong> across a diverse range of agentic deep research benchmarks, including Humanity's Last Exam and BrowseComp, demonstrating its robust capabilities. The project is also open-sourced, providing the community with the model, framework, and complete solutions.</p>

<h2>Critical Evaluation of Tongyi DeepResearch</h2>

<h3>Strengths in Agentic LLM Development</h3>
<p>The article presents several compelling strengths, particularly in its methodological approach to developing advanced agentic LLMs. A significant contribution is the novel two-stage agent training pipeline, encompassing agentic continual pre-training (CPT) with synthesized behavior data, followed by post-training via Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). The reliance on <strong>LLM-generated synthetic data</strong> is a crucial innovation, effectively addressing the scarcity of natural data for scaling agentic capabilities. Furthermore, the multi-environment strategy, utilizing Prior World, Simulated, and Real-world settings, provides a robust and stable training ground. Tongyi DeepResearch's formulation, defining Thought, Action, and Observation, coupled with ReAct and Markovian context management, is well-suited for long-horizon tasks. The model's demonstrated <strong>state-of-the-art performance</strong> across numerous benchmarks, enhanced by a "Heavy Mode" for parallel research and synthesis, underscores its effectiveness. The decision to <strong>open-source</strong> the model and framework is also a substantial benefit to the broader scientific community.</p>

<h3>Identified Limitations and Future Directions</h3>
<p>While Tongyi DeepResearch showcases impressive capabilities, the article also acknowledges certain limitations, primarily concerning <strong>context length</strong>. The analysis indicates that while larger contexts yield higher rewards, smaller contexts demonstrate greater efficiency in learning, suggesting an ongoing optimization challenge. This highlights a trade-off that future research could address to balance performance with computational demands. The authors advocate for the development of smaller, more efficient models, pointing towards a future where agentic LLMs can operate effectively with reduced resource requirements. This focus on efficiency and the aspiration for <strong>general-purpose, open-source agent foundation models</strong> for autonomous AI represent clear pathways for continued innovation.</p>

<h3>Implications for Autonomous AI Research</h3>
<p>Tongyi DeepResearch represents a significant stride in the field of <strong>agentic LLMs</strong>, offering a powerful tool for automating and enhancing complex information-seeking research. Its innovative training framework and reliance on scalable synthetic data provide a blueprint for developing future autonomous AI systems. The open-sourcing of this model is poised to accelerate research and development within the community, fostering collaborative advancements in AI agents capable of deep, independent inquiry. This work has profound implications for scientific discovery, potentially streamlining research processes and enabling new frontiers in knowledge generation through highly capable, autonomous AI.</p>
  </div>
  <div id = "keywordsWapper" class="mb-4">
    <h3>Keywords</h3>
    <ul>
  <li>agentic large language model for deep research</li><li> long‚Äëhorizon information‚Äëseeking tasks</li><li> agentic mid‚Äëtraining fine‚Äëtuning</li><li> agentic post‚Äëtraining autonomous reasoning</li><li> automatic data synthesis pipeline without human annotation</li><li> parameter‚Äëefficient activation (3.3‚ÄØB per token)</li><li> customized training environments for stable interactions</li><li> state‚Äëof‚Äëthe‚Äëart performance on BrowseComp benchmark</li><li> open‚Äësource research agent framework and model</li><li> scalable reasoning across web‚Äëbased tasks</li><li> xbench‚ÄëDeepSearch deep search evaluation</li><li> Humanity‚Äôs Last Exam LLM benchmark</li><li> WebWalkerQA question‚Äëanswering benchmark</li><li> FRAMES knowledge integration benchmark</li><li> low‚Äëactivation per token LLM architecture.</li>
</ul>

  </div>
  <div id="linktoWebsiteWrapper">
   <p>
Read article comprehensive review in Paperium.net:
<a href="https://paperium.net/article/en/787/tongyi-deepresearch-technical-report" target="_blank" title=" Tongyi DeepResearch Technical Report">
    Tongyi DeepResearch Technical Report
</a>
</p> 
 
</div>

<p class="mt-5">
    ü§ñ This analysis and review was primarily generated and structured by an AI . The content is provided for informational and quick-review purposes.
</p>
            <div class="my-5" id=similarWrapper>
       <h2 class="lead text-center mb-5" id="similarh2">Paperium AI Analysis & Review of Latest Scientific Research Articles </h2>
<h3 id="similarh3"> More Artificial Intelligence Article Reviews </h3>
       <div class="row row-cols-1 row-cols-md-2 row-cols-lg-3 g-4" id="articles-container" itemscope itemtype="https://schema.org/ItemList" >

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/687_0f75b6e8-1954-4286-8624-465ee0c94fb7.jpg" class="card-img-top" alt="Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive
Texture Infilling" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Shuhong Zheng
          </div>
          <div class="article-meta-text">
            29 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/781-Track-Inpaint-Resplat-Subject-driven-3D-and-4D-Generation-with-Progressive-Texture-Infilling/index.html"  title="Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive
Texture Infilling">
          <h3 class="card-title pb-2" itemprop="headline">Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive
Texture Infilling</h3>
        </a>
        <a 
          href="/paperium-articles/articles/781-Track-Inpaint-Resplat-Subject-driven-3D-and-4D-Generation-with-Progressive-Texture-Infilling/index.html"
          title="Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive
Texture Infilling"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/792_dbd117ba-3208-4e83-b6a9-792bce6c4790.jpg" class="card-img-top" alt="FullPart: Generating each 3D Part at Full Resolution" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Lihe Ding
          </div>
          <div class="article-meta-text">
            01 Nov 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/886-FullPart-Generating-each-3D-Part-at-Full-Resolution/index.html"  title="FullPart: Generating each 3D Part at Full Resolution">
          <h3 class="card-title pb-2" itemprop="headline">FullPart: Generating each 3D Part at Full Resolution</h3>
        </a>
        <a 
          href="/paperium-articles/articles/886-FullPart-Generating-each-3D-Part-at-Full-Resolution/index.html"
          title="FullPart: Generating each 3D Part at Full Resolution"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/693_51af6959-0309-4513-a33a-37380bf7265d.jpg" class="card-img-top" alt="Tongyi DeepResearch Technical Report" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Tongyi DeepResearch Team
          </div>
          <div class="article-meta-text">
            29 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/787-Tongyi-DeepResearch-Technical-Report/index.html"  title="Tongyi DeepResearch Technical Report">
          <h3 class="card-title pb-2" itemprop="headline">Tongyi DeepResearch Technical Report</h3>
        </a>
        <a 
          href="/paperium-articles/articles/787-Tongyi-DeepResearch-Technical-Report/index.html"
          title="Tongyi DeepResearch Technical Report"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/654_ff657b86-b6c3-46c0-8a8b-38000f478303.jpg" class="card-img-top" alt="A Survey of Data Agents: Emerging Paradigm or Overstated Hype?" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Yizhang Zhu
          </div>
          <div class="article-meta-text">
            31 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/821-A-Survey-of-Data-Agents-Emerging-Paradigm-or-Overstated-Hype/index.html"  title="A Survey of Data Agents: Emerging Paradigm or Overstated Hype?">
          <h3 class="card-title pb-2" itemprop="headline">A Survey of Data Agents: Emerging Paradigm or Overstated Hype?</h3>
        </a>
        <a 
          href="/paperium-articles/articles/821-A-Survey-of-Data-Agents-Emerging-Paradigm-or-Overstated-Hype/index.html"
          title="A Survey of Data Agents: Emerging Paradigm or Overstated Hype?"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/813_cb956f20-4f49-4b0a-80d0-569221b41689.jpg" class="card-img-top" alt="PORTool: Tool-Use LLM Training with Rewarded Tree" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Feijie Wu
          </div>
          <div class="article-meta-text">
            02 Nov 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/904-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree/index.html"  title="PORTool: Tool-Use LLM Training with Rewarded Tree">
          <h3 class="card-title pb-2" itemprop="headline">PORTool: Tool-Use LLM Training with Rewarded Tree</h3>
        </a>
        <a 
          href="/paperium-articles/articles/904-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree/index.html"
          title="PORTool: Tool-Use LLM Training with Rewarded Tree"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/743_ae2de635-3e73-4170-99c7-ea4e50e6704a.jpg" class="card-img-top" alt="RegionE: Adaptive Region-Aware Generation for Efficient Image Editing" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Pengtao Chen
          </div>
          <div class="article-meta-text">
            31 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/843-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing/index.html"  title="RegionE: Adaptive Region-Aware Generation for Efficient Image Editing">
          <h3 class="card-title pb-2" itemprop="headline">RegionE: Adaptive Region-Aware Generation for Efficient Image Editing</h3>
        </a>
        <a 
          href="/paperium-articles/articles/843-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing/index.html"
          title="RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

       </div>
   </div>
        </article>
        
 

     
    </div>
    </div>
    </div>
  <script src="/paperium-articles/assets/script1.js"></script>
 
</body>
</html>