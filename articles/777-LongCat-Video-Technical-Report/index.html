<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="/paperium-articles/assets/article_detail.css?ver=2"> 
    <link rel="stylesheet" href="/paperium-articles/assets/StyleSheet.css?ver=2" >

<title>LongCat-Video Technical Report</title>

<meta name="keywords" content="LongCat-Video 13.6B parameter model,  Diffusion Transformer (DiT) video generation,  Text-to-Video diffusion models,  Image-to-Video generation with b">

<meta name="description" content="LongCat-Video 13.6B parameter model,  Diffusion Transformer (DiT) video generation,  Text-to-Video diffusion models,  Image-to-Video generation with b">

</head>
<body>
    <div class="container my-5">
        <div class="row">
    <div class="header-nav text-center">
        <a href="/paperium-articles/">Home</a> 
        <span class="separator">|</span> 
        <a href="#">Article</a> 
    </div>

    <div class="article-container">
        <div class="article-tags-row">
            <span class="article-category-badge ">Artificial Intelligence</span>
            <span class="article-category-badge">arXiv</span>
        </div>
        <article id="MainArticle" class="details-body customeArticle" itemscope itemtype="https://schema.org/Article">
            <h1 itemprop="headline" class="article-title">
                LongCat-Video Technical Report
            </h1> 

            <div class="article-author-row">
                <span class="author-name">
                    <img src="/paperium-articles/assets/avatar.png" alt="Author" class="author-icon"> 
                    <span class="authorList">
                        Meituan LongCat Team, Xunliang Cai, Qilong Huang, Zhuoliang Kang, Hongyu Li, Shijun Liang, Liya Ma, Siyu Ren, Xiaoming Wei, Rixu Xie, Tong Zhang
                    </span>
                </span>
            </div>
            <p class="publish-date">
                              29 Oct 2025&nbsp;&nbsp;&nbsp;&nbsp;  3 min read 
            </p>
                            <div class="article-image-wrapper">
                                <figure id="authorsImage"  >
                                    <img class="article-main-image" itemprop="image" src="https://paperium.net/Media/Articles/img/682_9b077f4d-d00a-4a0e-9f9e-17a113c28170.jpg" class="object-center object-cover" 
                                    alt="undefined" >
                                </figure>
                                 <p class="image-caption">AI-generated image, based on the article abstract</p>
                            </div>

            <div id="QuickInsightWapper" class="mb-4"  itemprop="abstract">
               <h2>
     
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Quick Insight
       </h2>
            <h3>AI Breakthrough: Creating Minutes‚ÄëLong Videos in Minutes</h3>
<p>
Ever imagined a computer that could spin a whole movie‚Äëlength clip in the time it takes to brew coffee? <strong>Scientists have unveiled</strong> a new AI model called LongCat‚ÄëVideo that does exactly that. Built on a clever ‚Äúdiffusion transformer‚Äù engine, this system can turn a short text prompt, a single picture, or even a brief clip into a smooth, high‚Äëdefinition video that lasts for minutes‚Äîall without the usual lag. Think of it like a master chef who can prepare a multi‚Äëcourse feast by adding ingredients step‚Äëby‚Äëstep, first sketching the outline and then filling in the details, making the process fast and efficient. <strong>LongCat‚ÄëVideo keeps every frame in perfect sync</strong>, so the motion feels natural, and it does so at 720p and 30 frames per second in just a few minutes. This <strong>breakthrough</strong> opens doors for creators, educators, and anyone who dreams of bringing stories to life without waiting hours for rendering. The future of video may soon be as instant as a click. üåü
</p>
           </div> 
           <hr/>
  <div id="ShortReviewWapper" class="mb-4" itemprop="articleBody">
 <h2>
     <svg fill="#000000" width="30px" height="30px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg">
<title>paper-plane</title>
<path d="M0 14.016l9.216 6.912 18.784-16.928-14.592 20.064 10.592 7.936 8-32zM8 32l6.016-4-6.016-4v8z"></path>
</svg>
   Short Review
       </h2>
        <h2>Advancing Long Video Generation with LongCat-Video: A Scientific Review</h2>

<p>This scientific analysis delves into <strong>LongCat-Video</strong>, a substantial 13.6 billion parameter foundational model designed for efficient, high-quality long video generation, marking a significant stride towards developing comprehensive <strong>world models</strong>. Utilizing a unified <strong>Diffusion Transformer (DiT) architecture</strong>, LongCat-Video adeptly handles Text-to-Video, Image-to-Video, and Video-Continuation tasks within a single framework. Its innovative approach integrates a coarse-to-fine generation strategy across temporal and spatial axes, alongside <strong>Block Sparse Attention</strong>, to achieve remarkable inference efficiency for minutes-long, 720p, 30fps videos. The model's robust performance is further enhanced by a multi-reward <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> training paradigm, incorporating Gradient Reweighting Policy Optimization (GRPO) to ensure stability and superior output quality.</p>

<h2>Critical Evaluation</h2>

<h3>Strengths</h3>
<p>LongCat-Video presents several compelling strengths that position it as a leading model in generative AI. Its <strong>unified architecture</strong> for diverse video generation tasks streamlines development and application, offering versatility. The model's exceptional efficiency in generating <strong>long, temporally coherent videos</strong> is a major breakthrough, achieved through sophisticated techniques like coarse-to-fine generation and <strong>Block Sparse Attention</strong>, which significantly accelerate high-resolution inference. The comprehensive data curation pipeline, coupled with a multi-stage training process involving Supervised Fine-Tuning (SFT) and multi-reward RLHF with GRPO, ensures both high quality and training stability. Furthermore, the extensive evaluation protocol, encompassing both human and automatic benchmarks, rigorously validates its competitive performance against state-of-the-art models. The public availability of its code and model weights is a crucial contribution, fostering transparency and accelerating research in the field.</p>

<h3>Weaknesses</h3>
<p>While highly innovative, LongCat-Video does present areas for potential refinement. The complexity of its multi-stage training pipeline, involving advanced concepts like Flow Matching, LoRA, and fixed stochastic timestep SDE sampling, could pose a considerable barrier for researchers with limited computational resources or specialized expertise, despite the open-source release. Additionally, the evaluation identified specific areas for improvement, particularly concerning <strong>image and motion alignment</strong>, suggesting that while overall quality is high, subtle inconsistencies might still occur. Although efficient, generating "minutes-long videos within minutes" still implies substantial computational demands for large-scale or continuous video production, highlighting the ongoing challenge of scaling generative models.</p>

<h3>Implications</h3>
<p>The introduction of LongCat-Video carries significant implications for the future of generative AI and its applications. Its ability to produce high-quality, temporally coherent long videos efficiently opens new avenues for content creation, virtual reality, and advanced simulation environments. By laying a strong foundation for <strong>world models</strong>, LongCat-Video pushes the boundaries of AI's capacity to understand and generate dynamic, complex sequences. The sophisticated training methodologies, particularly the multi-reward RLHF and efficiency techniques, offer valuable insights that could inspire advancements across various generative model architectures. Its open-source nature is poised to democratize access to cutting-edge video generation technology, fostering collaborative innovation and accelerating the pace of research globally.</p>

<h2>Conclusion</h2>
<p>LongCat-Video represents a pivotal advancement in the domain of video generation, effectively addressing the critical challenge of producing <strong>efficient, high-quality long videos</strong>. Its innovative architectural design, robust training methodologies, and demonstrated performance solidify its position as a foundational model. Despite minor areas for refinement, its contributions to efficiency, temporal coherence, and the pursuit of world models are substantial, making it an invaluable resource for the scientific community and a significant step forward in generative AI.</p>
  </div>
  <div id = "keywordsWapper" class="mb-4">
    <h3>Keywords</h3>
    <ul>
  <li>LongCat-Video 13.6B parameter model</li><li> Diffusion Transformer (DiT) video generation</li><li> Text-to-Video diffusion models</li><li> Image-to-Video generation with block sparse attention</li><li> Video-Continuation pretraining for minutes-long videos</li><li> Coarse-to-fine temporal‚Äëspatial generation strategy</li><li> Efficient 720p 30fps video inference</li><li> Multi‚Äëreward RLHF for video generation</li><li> World‚Äëmodel video synthesis</li><li> Block sparse attention for high‚Äëresolution video</li><li> Long video temporal coherence techniques</li><li> Open‚Äësource foundational video model</li><li> Unified architecture for multiple video tasks</li>
</ul>

  </div>
  <div id="linktoWebsiteWrapper">
   <p>
Read article comprehensive review in Paperium.net:
<a href="https://paperium.net/article/en/777/longcat-video-technical-report" target="_blank" title=" LongCat-Video Technical Report">
    LongCat-Video Technical Report
</a>
</p> 
 
</div>

<p class="mt-5">
    ü§ñ This analysis and review was primarily generated and structured by an AI . The content is provided for informational and quick-review purposes.
</p>
            <div class="my-5" id=similarWrapper>
       <h2 class="lead text-center mb-5" id="similarh2">Paperium AI Analysis & Review of Latest Scientific Research Articles </h2>
<h3 id="similarh3"> More Artificial Intelligence Article Reviews </h3>
       <div class="row row-cols-1 row-cols-md-2 row-cols-lg-3 g-4" id="articles-container" itemscope itemtype="https://schema.org/ItemList" >

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/819_a752c393-219e-400b-9f5b-be066c4bf03f.jpg" class="card-img-top" alt="L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Jiyu Cui
          </div>
          <div class="article-meta-text">
            02 Nov 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/909-L2M3OF-A-Large-Language-Multimodal-Model-for-Metal-Organic-Frameworks/index.html"  title="L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks">
          <h3 class="card-title pb-2" itemprop="headline">L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks</h3>
        </a>
        <a 
          href="/paperium-articles/articles/909-L2M3OF-A-Large-Language-Multimodal-Model-for-Metal-Organic-Frameworks/index.html"
          title="L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/651_eec9c8a6-3a92-4fa7-98dd-6fefb86bc9cc.jpg" class="card-img-top" alt="ReCode: Unify Plan and Action for Universal Granularity Control" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Zhaoyang Yu
          </div>
          <div class="article-meta-text">
            29 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/751-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control/index.html"  title="ReCode: Unify Plan and Action for Universal Granularity Control">
          <h3 class="card-title pb-2" itemprop="headline">ReCode: Unify Plan and Action for Universal Granularity Control</h3>
        </a>
        <a 
          href="/paperium-articles/articles/751-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control/index.html"
          title="ReCode: Unify Plan and Action for Universal Granularity Control"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/548_2ecd9a26-797c-418a-b429-f85765f24dfa.jpg" class="card-img-top" alt="Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Jiahao Meng
          </div>
          <div class="article-meta-text">
            24 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/657-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence/index.html"  title="Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence">
          <h3 class="card-title pb-2" itemprop="headline">Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</h3>
        </a>
        <a 
          href="/paperium-articles/articles/657-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence/index.html"
          title="Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/633_cbf0fc0b-88c6-43d8-bc4a-c1f0373ffc5e.jpg" class="card-img-top" alt="Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance
Boost" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Runzhe Zhan
          </div>
          <div class="article-meta-text">
            27 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/739-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost/index.html"  title="Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance
Boost">
          <h3 class="card-title pb-2" itemprop="headline">Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance
Boost</h3>
        </a>
        <a 
          href="/paperium-articles/articles/739-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost/index.html"
          title="Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance
Boost"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/634_40b7f7e3-e0d3-457a-b221-5d3d7f88cf20.jpg" class="card-img-top" alt="ARC-Encoder: learning compressed text representations for large language models" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Hippolyte Pilchen
          </div>
          <div class="article-meta-text">
            27 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/740-ARC-Encoder-learning-compressed-text-representations-for-large-language-models/index.html"  title="ARC-Encoder: learning compressed text representations for large language models">
          <h3 class="card-title pb-2" itemprop="headline">ARC-Encoder: learning compressed text representations for large language models</h3>
        </a>
        <a 
          href="/paperium-articles/articles/740-ARC-Encoder-learning-compressed-text-representations-for-large-language-models/index.html"
          title="ARC-Encoder: learning compressed text representations for large language models"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

 <article data-ns-animate="" data-delay="0.3" class="group"
             itemscope itemtype="https://schema.org/Article">
  <div class="col">
    <div class="article-card">
      <img src="https://paperium.net/Media/Articles/img/517_c37fd5cc-a5d3-493b-a64f-57cfb56e2c8a.jpg" class="card-img-top" alt="Language Models are Injective and Hence Invertible" itemprop="image">
      <div class="article-card-body">
        <div class="article-meta">
          <div>
            <span class="article-category-badge">
              Artificial Intelligence
            </span>
          </div>
          <div class="article-meta-text">
            Giorgos Nikolaou
          </div>
          <div class="article-meta-text">
            24 Oct 2025
          </div>
        </div>
        <a href="/paperium-articles/articles/627-Language-Models-are-Injective-and-Hence-Invertible/index.html"  title="Language Models are Injective and Hence Invertible">
          <h3 class="card-title pb-2" itemprop="headline">Language Models are Injective and Hence Invertible</h3>
        </a>
        <a 
          href="/paperium-articles/articles/627-Language-Models-are-Injective-and-Hence-Invertible/index.html"
          title="Language Models are Injective and Hence Invertible"
          target="_blank"
          class="btn btn-read-more mt-2"
          role="button" itemprop="url">
          Read Article
        </a>
      </div>
    </div>
  </div>
  </article>

       </div>
   </div>
        </article>
        
 

     
    </div>
    </div>
    </div>
  <script src="/paperium-articles/assets/script1.js"></script>
 
</body>
</html>